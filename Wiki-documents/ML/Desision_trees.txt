„ерево решений

’екущаЯ версиЯ страницы пока не проверЯлась опытными участниками и может значительно отличатьсЯ от версии, проверенной 10 декабрЯ 2015; проверки требуют 11 правок.
„ерево принЯтиЯ решений (также может называтьсЯ деревом классификации или регрессионным деревом) С средство поддержки принЯтиЯ решений, использующеесЯ в статистике и анализе данных длЯ прогнозных моделей. ‘труктура дерева представлЯет собой ЗлистьЯИ и ЗветкиИ. Ќа ребрах (ЗветкахИ) дерева решениЯ записаны атрибуты, от которых зависит целеваЯ функциЯ, в ЗлистьЯхИ записаны значениЯ целевой функции, а в остальных узлах С атрибуты, по которым различаютсЯ случаи. —тобы классифицировать новый случай, надо спуститьсЯ по дереву до листа и выдать соответствующее значение. Џодобные деревьЯ решений широко используютсЯ в интеллектуальном анализе данных. –ель состоит в том, чтобы создать модель, котораЯ предсказывает значение целевой переменной на основе нескольких переменных на входе.

CART tree titanic survivors.png

Љаждый лист представлЯет собой значение целевой переменной, измененной в ходе движениЯ от корнЯ по листу. Љаждый внутренний узел соответствует одной из входных переменных. „ерево может быть также ЗизученоИ разделением исходных наборов переменных на подмножества, основанные на тестировании значений атрибутов. ќто процесс, который повторЯетсЯ на каждом из полученных подмножеств. ђекурсиЯ завершаетсЯ тогда, когда подмножество в узле имеет те же значениЯ целевой переменной, таким образом, оно не добавлЯет ценности длЯ предсказаний. Џроцесс, идущий Зсверху внизИ, индукциЯ деревьев решений (TDIDT)[1], ЯвлЯетсЯ примером поглощающего ЗжадногоИ алгоритма, и на сегоднЯшний день ЯвлЯетсЯ наиболее распространенной стратегией деревьев решений длЯ данных, но это не единственнаЯ возможнаЯ стратегиЯ. ‚ интеллектуальном анализе данных, деревьЯ решений могут быть использованы в качестве математических и вычислительных методов, чтобы помочь описать, классифицировать и обобщить набор данных, которые могут быть записаны следующим образом:

{\displaystyle (x,Y)=(x_{1},x_{2},x_{3}\dots x_{k},Y)} (x,Y)=(x_{1},x_{2},x_{3}\dots x_{k},Y)
‡ависимаЯ переменнаЯ Y ЯвлЯетсЯ целевой переменной, которую необходимо проанализировать, классифицировать и обобщить. ‚ектор {\displaystyle x} x состоит из входных переменных {\displaystyle x_{1}} x_{1}, {\displaystyle x_{2}} x_{2}, {\displaystyle x_{3}} x_{3} и т. д., которые используютсЯ длЯ выполнениЯ этой задачи.

‘одержание
1	Ћсновные определениЯ
2	’ипологиЯ деревьев
3	Ђлгоритмы построениЯ дерева
4	„остоинства метода
5	Ќедостатки метода
6	ђегулирование глубины дерева
6.1	Њетоды регулированиЯ
7	Џример задачи
8	‘м. также
9	ЏримечаниЯ
10	‘сылки
11	‹итература
Ћсновные определениЯ
Wikitext-ru.svg
ќту статью следует викифицировать.
Џожалуйста, оформите еЮ согласно правилам оформлениЯ статей.
Џри анализе решений Здерево решенийИ используютсЯ как визуальный и аналитический инструмент поддержки принЯтиЯ решений, где рассчитываютсЯ ожидаемые значениЯ (или ожидаемаЯ полезность) конкурирующих альтернатив.

„ерево решений состоит из трЮх типов узлов:

“злы решениЯ С обычно представлены квадратами
‚ероЯтностные узлы С представлЯютсЯ в виде круга
‡амыкающие узлы С представлЯютсЯ в виде треугольника
Decision-Tree-Elements.png

Ќа рисунке, представленном выше, дерево решений следует читать слева направо. „ерево решений не может содержать в себе циклические элементы, то есть каждый новый лист впоследствии может лишь расщеплЯтьсЯ, отсутствуют сходЯщиесЯ пути. ’аким образом, при конструировании дерева вручную, мы можем столкнутьсЯ с проблемой его размерности, поэтому, как правило, дерево решениЯ мы можем получить с помощью специализированного программного обеспечениЯ. Ћбычно дерево решений представлЯетсЯ в виде символической схемы, благодарЯ которой его проще воспринимать и анализировать.

Decision tree using flow chart symbols.jpg

’ипологиЯ деревьев
„еревьЯ решений, используемые в Data Mining, бывают двух основных типов:

„ерево длЯ классификации, когда предсказываемый результат ЯвлЯетсЯ классом, к которому принадлежат данные;
„ерево длЯ регрессии, когда предсказываемый результат можно рассматривать как вещественное число (например, цена на дом, или продолжительность пребываниЯ пациента в больнице).
“помЯнутые выше термины впервые были введены Ѓрейманом и др.[2] Џеречисленные типы имеют некоторые сходства (рекурсивный алгоритмы построениЯ), а также некоторые различиЯ, такие, как критерии выбора разбиениЯ в каждом узле.[2]

Ќекоторые методы позволЯют построить более одного дерева решений (ансамбли деревьев решений):

Ѓэггинг над деревьЯми решений, наиболее ранний подход. ‘троит несколько деревьев решений, неоднократно интерполируЯ данные с заменой (бутстреп), и в качестве консенсусного ответа выдаЮт результат голосованиЯ деревьев (их средний прогноз);[3]
Љлассификатор З‘лучайный лесИ основан на бэггинге, однако в дополнение к нему случайным образом выбирает подмножество признаков в каждом узле, с целью сделать деревьЯ более независимыми;
Ѓустинг над деревьЯми может быть использован длЯ задач как регрессии, так и классификации.[4] Ћдна из реализаций бустинга над деревьЯми, алгоритм XGBoost, неоднократно использовалсЯ победителЯми соревнований по анализу данных.
З‚ращение лесаИ С деревьЯ, в которых каждое дерево решений анализируетсЯ первым применением метода главных компонент (PCA) на случайные подмножества входных функций.[5]
Ђлгоритмы построениЯ дерева
ЋбщаЯ схема построениЯ дерева принЯтиЯ решений по тестовым примерам выглЯдит следующим образом:

‚ыбираем очередной атрибут {\displaystyle Q} Q, помещаем его в корень.
„лЯ всех его значений {\displaystyle i} i:
ЋставлЯем из тестовых примеров только те, у которых значение атрибута {\displaystyle Q} Q равно {\displaystyle i} i
ђекурсивно строим дерево в этом потомке
Ћсновной вопрос состоит в том, как выбирать очередной атрибут.

…сть различные способы выбирать очередной атрибут:

Ђлгоритм ID3, где выбор атрибута происходит на основании прироста информации (англ. Gain), либо на основании критериЯ „жини.
Ђлгоритм C4.5 (улучшеннаЯ версиЯ ID3), где выбор атрибута происходит на основании нормализованного прироста информации (англ. Gain Ratio).
Ђлгоритм CART и его модификации С IndCART, DB-CART.
Ђвтоматический детектор взаимодействиЯ •и-квадрат (CHAID). ‚ыполнЯет многоуровневое разделение при расчете классификации деревьев;[6]
MARS: расширЯет деревьЯ решений длЯ улучшениЯ обработки цифровых данных.
Ќа практике в результате работы этих алгоритмов часто получаютсЯ слишком детализированные деревьЯ, которые при их дальнейшем применении дают много ошибок. ќто свЯзано с Явлением переобучениЯ. „лЯ сокращениЯ деревьев используетсЯ отсечение ветвей (англ. pruning).

„остоинства метода
‘реди прочих методов Data Mining, метод дерева принЯтиЯ решений имеет несколько достоинств:

Џрост в понимании и интерпретации. ‹юди способны интерпретировать результаты модели дерева принЯтиЯ решений после краткого объЯснениЯ
Ќе требует подготовки данных. Џрочие техники требуют нормализации данных, добавлениЯ фиктивных переменных, а также удалениЯ пропущенных данных.
‘пособен работать как с категориальными, так и с интервальными переменными. Џрочие методы работают лишь с теми данными, где присутствует лишь один тип переменных. Ќапример, метод отношений может быть применен только на номинальных переменных, а метод нейронных сетей только на переменных, измеренных по интервальной шкале.
€спользует модель Збелого ЯщикаИ. …сли определеннаЯ ситуациЯ наблюдаетсЯ в модели, то еЮ можно объЯснить при помощи булевой логики. Џримером Зчерного ЯщикаИ может быть искусственнаЯ нейроннаЯ сеть, так как результаты данной модели поддаютсЯ объЯснению с трудом.
ЏозволЯет оценить модель при помощи статистических тестов. ќто дает возможность оценить надежность модели.
џвлЯетсЯ надежным методом. Њетод хорошо работает даже в том случае, если были нарушены первоначальные предположениЯ, включенные в модель.
ЏозволЯет работать с большим объемом информации без специальных подготовительных процедур. „анный метод не требует специального оборудованиЯ длЯ работы с большими базами данных.
Ќедостатки метода
Џроблема получениЯ оптимального дерева решений ЯвлЯетсЯ NP-полной с точки зрениЯ некоторых аспектов оптимальности даже длЯ простых задач[7][8]. ’аким образом, практическое применение алгоритма деревьев решений основано на эвристических алгоритмах, таких как алгоритм ЗжадностиИ, где единственно оптимальное решение выбираетсЯ локально в каждом узле. ’акие алгоритмы не могут обеспечить оптимальность всего дерева в целом.
’е, кто изучает метод дерева принЯтиЯ решений, могут создавать слишком сложные конструкции, которые недостаточно полно представлЯют данные. „аннаЯ проблема называетсЯ переобучением[9]. „лЯ того, чтобы избежать данной проблемы, необходимо использовать Њетод ЗрегулированиЯ глубины дереваИ.
‘уществуют концепты, которые сложно понЯть из модели, так как модель описывает их сложным путЮм. „анное Явление может быть вызвано проблемами XOR, четности или мультиплексарности. ‚ этом случае мы имеем дело с непомерно большими деревьЯми. ‘уществует несколько подходов решениЯ данной проблемы, например, попытка изменить репрезентацию концепта в модели (составление новых суждений)[10], или использование алгоритмов, которые более полно описывают и репрезентируют концепт (например, метод статистических отношений, индуктивнаЯ логика программированиЯ).
„лЯ данных, которые включают категориальные переменные с большим набором уровней (закрытий), больший информационный вес присваиваетсЯ тем атрибутам, которые имеют большее количество уровней.[11]
ђегулирование глубины дерева
ђегулирование глубины дерева С это техника, котораЯ позволЯет уменьшать размер дерева решений, удалЯЯ участки дерева, которые имеют маленький вес.

Ћдин из вопросов, который возникает в алгоритме дерева решений С это оптимальный размер конечного дерева. ’ак, небольшое дерево может не охватить ту или иную важную информацию о выборочном пространстве. ’ем не менее, трудно сказать, когда алгоритм должен остановитьсЯ, потому что невозможно спрогнозировать, добавление какого узла позволит значительно уменьшить ошибку. ќта проблема известна как Зэффект горизонтаИ. ’ем не менее, общаЯ стратегиЯ ограничениЯ дерева сохранЯетсЯ, то есть удаление узлов реализуетсЯ в случае, если они не дают дополнительной информации[12].

Ќеобходимо отметить, что регулирование глубины дерева должно уменьшить размер обучающей модели дерева без уменьшениЯ точности еЮ прогноза или с помощью перекрестной проверки. …сть много методов регулированиЯ глубины дерева, которые отличаютсЯ измерением оптимизации производительности.

Њетоды регулированиЯ
‘окращение дерева может осуществлЯтьсЯ сверху вниз или снизу вверх. ‘верху вниз С обрезка начинаетсЯ с корнЯ, снизу вверх С сокращаетсЯ число листьев дерева. Ћдин из простейших методов регулированиЯ С уменьшение ошибки ограничениЯ дерева. ЌачинаЯ с листьев, каждый узел заменЯетсЯ на самый популЯрный класс. …сли изменение не влиЯет на точность предсказаниЯ, то оно сохранЯетсЯ.