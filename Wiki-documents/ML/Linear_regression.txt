‹инейнаЯ регрессиЯ

‹инейнаЯ регрессиЯ (англ. Linear regression) С используемаЯ в статистике регрессионнаЯ модель зависимости одной (объЯснЯемой, зависимой) переменной {\displaystyle y} y от другой или нескольких других переменных (факторов, регрессоров, независимых переменных) {\displaystyle x} x с линейной функцией зависимости.

Њодель линейной регрессии ЯвлЯетсЯ часто используемой и наиболее изученной в эконометрике. Ђ именно изучены свойства оценок параметров, получаемых различными методами при предположениЯх о вероЯтностных характеристиках факторов, и случайных ошибок модели. Џредельные (асимптотические) свойства оценок нелинейных моделей также выводЯтсЯ исходЯ из аппроксимации последних линейными моделЯми. Ќеобходимо отметить, что с эконометрической точки зрениЯ более важное значение имеет линейность по параметрам, чем линейность по факторам модели.

‘одержание
1	Ћпределение
1.1	ЏарнаЯ и множественнаЯ регрессиЯ
2	Џримеры
2.1	Њодель затрат организации (без указаниЯ случайной ошибки)
2.2	ЏростейшаЯ модель потребительских расходов (Љейнс)
3	Њатричное представление
4	ЉлассическаЯ линейнаЯ регрессиЯ
5	Њетоды оценки
6	‘м. также
7	ЏримечаниЯ
8	‹итература
Ћпределение
ђегрессионнаЯ модель

{\displaystyle y=f(x,b)+\varepsilon ,~E(\varepsilon )=0} y=f(x,b)+\varepsilon, ~E(\varepsilon)=0,
где {\displaystyle b} b С параметры модели, {\displaystyle \varepsilon } \varepsilon  С случайнаЯ ошибка модели; называетсЯ линейной регрессией, если функциЯ регрессии {\displaystyle f(x,b)} f(x,b) имеет вид

{\displaystyle f(x,b)=b_{0}+b_{1}x_{1}+b_{2}x_{2}+...+b_{k}x_{k}} f(x,b)=b_0+b_1 x_1+b_2 x_2+...+b_k x_k,
где {\displaystyle b_{j}} b_{j} С параметры (коэффициенты) регрессии, {\displaystyle x_{j}} x_{j} С регрессоры (факторы модели), k С количество факторов модели[1].

Љоэффициенты линейной регрессии показывают скорость изменениЯ зависимой переменной по данному фактору, при фиксированных остальных факторах (в линейной модели эта скорость постоЯнна):

{\displaystyle \forall j~b_{j}={\frac {\partial f}{\partial x_{j}}}=const} \forall j ~b_j=\frac {\partial f}{\partial x_j}=const
Џараметр {\displaystyle b_{0}} b_{0}, при котором нет факторов, называют часто константой. ”ормально С это значение функции при нулевом значении всех факторов. „лЯ аналитических целей удобно считать, что константа С это параметр при ЗфактореИ, равном 1 (или другой произвольной постоЯнной, поэтому константой называют также и этот ЗфакторИ). ‚ таком случае, если перенумеровать факторы и параметры исходной модели с учетом этого (оставив обозначение общего количества факторов С k), то линейную функцию регрессии можно записать в следующем виде, формально не содержащем константу:

{\displaystyle f(x,b)=b_{1}x_{1}+b_{2}x_{2}+\ldots +b_{k}x_{k}=\sum _{j=1}^{k}b_{j}x_{j}=x^{T}b} f(x,b)=b_1 x_1 + b_2 x_2 + \ldots + b_k x_k=\sum^k_{j=1}b_j x_j=x^Tb,
где {\displaystyle x^{T}=(x_{1},x_{2},...,x_{k})} x^T=(x_1,x_2,...,x_k) С вектор регрессоров, {\displaystyle b=(b_{1},b_{2},\ldots ,b_{k})^{T}} b=(b_1,b_2, \ldots,b_k)^T С вектор-столбец параметров (коэффициентов).

‹инейнаЯ модель может быть как с константой, так и без константы. ’огда в этом представлении первый фактор либо равен единице, либо ЯвлЯетсЯ обычным фактором соответственно.

ЏарнаЯ и множественнаЯ регрессиЯ
‚ частном случае, когда фактор единственный (без учЮта константы), говорЯт о парной или простейшей линейной регрессии:

{\displaystyle y_{t}=a+bx_{t}+\varepsilon _{t}} y_t=a+b x_t+\varepsilon_t
Љогда количество факторов (без учЮта константы) больше 1-го, то говорЯт о множественной регрессии.

Џримеры
Њодель затрат организации (без указаниЯ случайной ошибки)
{\displaystyle TC=FC+VC=FC+v\cdot Q} TC=FC+VC=FC+v \cdot Q
{\displaystyle TC} TC С общие затраты
{\displaystyle FC} FC С постоЯнные затраты (не зависЯщие от объЮма производства)
{\displaystyle VC} VC С переменные затраты, пропорциональные объЮму производства
{\displaystyle v} v С удельные или средние (на единицу продукции) переменные затраты
{\displaystyle Q} Q С объЮм производства.
ЏростейшаЯ модель потребительских расходов (Љейнс)
{\displaystyle C=a+bY+\varepsilon } C=a+bY+\varepsilon
{\displaystyle C} C С потребительские расходы
{\displaystyle Y} Y С располагаемый доход
{\displaystyle b} b С ЗпредельнаЯ склонность к потреблениюИ
{\displaystyle a} a С автономное (не зависЯщее от дохода) потребление.
Њатричное представление
Џусть дана выборка объЮмом n наблюдений переменных y и x. Ћбозначим t С номер наблюдениЯ в выборке. ’огда {\displaystyle y_{t}} y_{t} С значение переменной y в t-м наблюдении, {\displaystyle x_{tj}} x_{tj} С значение j-го фактора в t-м наблюдении. ‘оответственно, {\displaystyle x_{t}^{T}=(x_{t1},x_{t2},...,x_{tk})} x^T_t=(x_{t1},x_{t2},...,x_{tk}) С вектор регрессоров в t-м наблюдении. ’огда линейнаЯ регрессионнаЯ зависимость имеет место в каждом наблюдении:

{\displaystyle y_{t}=b_{1}x_{t1}+b_{2}x_{t2}+...+b_{k}x_{tk}=\sum _{j=1}^{k}b_{j}x_{tj}=x_{t}^{T}b+\varepsilon _{t}~,~E(\varepsilon _{t})=0~,~t=1..n} y_t=b_1 x_{t1}+b_2 x_{t2}+...+b_k x_{tk}=\sum^k_{j=1}b_j x_{tj}=x^T_t b+\varepsilon_t~,~E(\varepsilon_t)=0~,~t=1..n
‚ведЮм обозначениЯ:

{\displaystyle y={\begin{pmatrix}y_{1}\\y_{2}\\...\\y_{n}\\\end{pmatrix}}} {\displaystyle y={\begin{pmatrix}y_{1}\\y_{2}\\...\\y_{n}\\\end{pmatrix}}} С вектор наблюдений зависимой переменой y
{\displaystyle X={\begin{pmatrix}x_{11}&x_{12}&...&x_{1k}\\x_{21}&x_{22}&...&x_{2k}\\...\\x_{n1}&x_{n2}&...&x_{nk}\\\end{pmatrix}}} {\displaystyle X={\begin{pmatrix}x_{11}&x_{12}&...&x_{1k}\\x_{21}&x_{22}&...&x_{2k}\\...\\x_{n1}&x_{n2}&...&x_{nk}\\\end{pmatrix}}} С матрица факторов.
{\displaystyle \varepsilon ={\begin{pmatrix}\varepsilon _{1}\\\varepsilon _{2}\\...\\\varepsilon _{n}\\\end{pmatrix}}} {\displaystyle \varepsilon ={\begin{pmatrix}\varepsilon _{1}\\\varepsilon _{2}\\...\\\varepsilon _{n}\\\end{pmatrix}}} С вектор случайных ошибок.
’огда модель линейной регрессии можно представить в матричной форме:

{\displaystyle y=Xb+\varepsilon } y=Xb+\varepsilon 
ЉлассическаЯ линейнаЯ регрессиЯ
‚ классической линейной регрессии предполагаетсЯ, что нарЯду со стандартным условием {\displaystyle E(\varepsilon _{t})=0} E(\varepsilon _{t})=0 выполнены также следующие предположениЯ (условиЯ ѓаусса-Њаркова):

ѓомоскедастичность (постоЯннаЯ или одинаковаЯ дисперсиЯ) или отсутствие гетероскедастичности случайных ошибок модели: {\displaystyle V(\varepsilon _{t})=\sigma ^{2}=const} V(\varepsilon _{t})=\sigma ^{2}=const
Ћтсутствие автокоррелЯции случайных ошибок: {\displaystyle \forall i,j,~i\not =j~~cov(\varepsilon _{i},\varepsilon _{j})=0} \forall i,j,~ i \not = j ~~cov(\varepsilon_i,\varepsilon_j)=0
„анные предположениЯ в матричном представлении модели формулируютсЯ в виде одного предположениЯ о структуре ковариационной матрицы вектора случайных ошибок: {\displaystyle V(\varepsilon )=\sigma ^{2}I_{n}} V(\varepsilon)=\sigma^2 I_n

Џомимо указанных предположений, в классической модели факторы предполагаютсЯ детерминированными (нестохастическими). Љроме того, формально требуетсЯ, чтобы матрица {\displaystyle X} X имела полный ранг ( {\displaystyle k} k), то есть предполагаетсЯ, что отсутствует полнаЯ коллинеарность факторов.

Џри выполнении классических предположений обычный метод наименьших квадратов позволЯет получить достаточно качественные оценки параметров модели, а именно: они ЯвлЯютсЯ несмещЮнными, состоЯтельными и наиболее эффективными оценками.

Њетоды оценки
Њетод наименьших квадратов
Ћбобщенный метод наименьших квадратов
Њетод инструментальных переменных
Њетод максимального правдоподобиЯ
Њетод моментов
Ћбобщенный метод моментов
ЉвантильнаЯ регрессиЯ