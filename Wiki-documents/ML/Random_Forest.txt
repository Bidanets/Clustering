Random forest

Текущая версия страницы пока не проверялась опытными участниками и может значительно отличаться от версии, проверенной 14 января 2018; проверки требуют 5 правок.
Random forest (с англ. — «случайный лес») — алгоритм машинного обучения, предложенный Лео Брейманом[1][2] и Адель Катлер, заключающийся в использовании комитета (ансамбля) решающих деревьев. Алгоритм сочетает в себе две основные идеи: метод бэггинга Бреймана, и метод случайных подпространств, предложенный Tin Kam Ho. Алгоритм применяется для задач классификации, регрессии и кластеризации. Основная идея заключается в использовании большого ансамбля решающих деревьев, каждое из которых само по себе даёт очень невысокое качество классификации, но за счёт их большого количества результат получается хорошим.

Содержание
1	Алгоритм обучения классификатора
2	Оценка важности переменных
3	Достоинства
4	Недостатки
5	Использование в научных работах
6	Примечания
7	Литература
8	Ссылки
Алгоритм обучения классификатора
Пусть обучающая выборка состоит из N образцов, размерность пространства признаков равна M, и задан параметр m (в задачах классификации обычно {\displaystyle m\approx {\sqrt {M}}} m\approx {\sqrt  {M}}) как неполное количество признаков для обучения.

Наиболее распространённый способ построения деревьев комитета следующий (называется бэггинг (англ. bagging, сокращение от англ. bootstrap aggregation)):

Сгенерируем случайную подвыборку с повторениями размером N из обучающей выборки. (Таким образом, некоторые образцы попадут в неё два или более раза, а в среднем N* {\displaystyle (1-1/N)^{N}} {\displaystyle (1-1/N)^{N}}, а примерно N/e образцов не войдут в неё вообще). Те образцы, которые не попали в выборку, называются out-of-bag (неотобранные).
Построим решающее дерево, классифицирующее образцы данной подвыборки, причём в ходе создания очередного узла дерева будем выбирать набор признаков, на основе которых производится разбиение (не из всех M признаков, а лишь из m случайно выбранных). Выбор наилучшего из этих m признаков может осуществляться различными способами. В оригинальном коде Бреймана используется критерий Джини, применяющийся также в алгоритме построения решающих деревьев CART. В некоторых реализациях алгоритма вместо него используется критерий прироста информации.[3]
Дерево строится до полного исчерпания подвыборки и не подвергается процедуре прунинга (англ. pruning — отсечение ветвей) (в отличие от решающих деревьев, построенных по таким алгоритмам, как CART или C4.5).
Классификация объектов проводится путём голосования: каждое дерево комитета относит классифицируемый объект к одному из классов, и побеждает класс, за который проголосовало наибольшее число деревьев.

Оптимальное число деревьев подбирается таким образом, чтобы минимизировать ошибку классификатора на тестовой выборке. В случае её отсутствия, минимизируется оценка ошибки out-of-bag: тех образцов, которые не попали в обучающую подвыборку за счёт повторений (их примерно N/e ).

Оценка важности переменных
Случайные леса, получаемые в результате применения техник, описанных ранее, могут быть естественным образом использованы для оценки важности переменных в задачах регрессии и классификации. Следующий способ такой оценки был описан Breiman.

Первый шаг в оценке важности переменной в тренировочном наборе — тренировка случайного леса на этом наборе. Во время процесса построения модели для каждого элемента тренировочного набора называемая out-of-bag-ошибка (ошибка на неотобранных образцах). Затем для каждой сущности такая ошибка усредняется по всему случайному лесу.

Для того, чтобы оценить важность {\displaystyle j} j-го параметра после тренировки, значения {\displaystyle j} j-го параметра перемешиваются для всех записей тренировочного набора и out-of-bag-ошибка считается снова. Важность параметра оценивается путём усреднения по всем деревьям разности показателей out-of-bag-ошибок до и после перемешивания значений. При этом значения таких ошибок нормализуются на стандартное отклонение.

Параметры выборки, которые дают бо́льшие значения, считаются более важными для тренировочного набора. Метод имеет следующий потенциальный недостаток — для категориальных переменных с большим количеством значений метод склонен считать такие переменные более важными. Частичное перемешивание значений в этом случае может снижать влияние этого эффекта.[4][5] Из групп коррелирующих параметров, важность которых оказывается одинаковой, выбираются меньшие по численности группы.[6]

Достоинства
Способность эффективно обрабатывать данные с большим числом признаков и классов.
Нечувствительность к масштабированию (и вообще к любым монотонным преобразованиям) значений признаков.
Одинаково хорошо обрабатываются как непрерывные, так и дискретные признаки. Существуют методы построения деревьев по данным с пропущенными значениями признаков.
Существуют методы оценивания значимости отдельных признаков в модели.
Внутренняя оценка способности модели к обобщению (тест по неотобранным образцам out-of-bag).
Высокая параллелизуемость и масштабируемость.
Недостатки
Большой размер получающихся моделей. Требуется {\displaystyle O(K)} {\displaystyle O(K)} памяти для хранения модели, где {\displaystyle K} K — число деревьев.
Использование в научных работах
Алгоритм часто используется в научных работах из-за его преимуществ. Например, его можно использовать для оценки качества статей Википедии.